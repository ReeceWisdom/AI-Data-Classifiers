
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>mynb</title><meta name="generator" content="MATLAB 9.7"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2020-01-14"><meta name="DC.source" content="mynb.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><pre class="codeinput"><span class="keyword">classdef</span> mynb

    <span class="keyword">methods</span>(Static)

        <span class="keyword">function</span> m = fit(train_examples, train_labels)

            <span class="comment">% array of all unique training labels</span>
            m.unique_classes = unique(train_labels);
            <span class="comment">% calculates the number of the unique labels</span>
            m.n_classes = length(m.unique_classes);
            <span class="comment">% mean &amp; standard deviation used to generate the</span>
            <span class="comment">% 'Probability Density' / 'Normal Distribution'</span>
            m.means = {}; <span class="comment">% empty cell array to store the mean</span>
            m.stds = {}; <span class="comment">% empty cell array to store the standard deviation</span>

            <span class="comment">% loops through all of the unique training labels</span>
            <span class="keyword">for</span> i = 1:m.n_classes

                <span class="comment">% extracts first label from the array of unique labels</span>
				this_class = m.unique_classes(i);
                <span class="comment">% extracts all training examples which belong to that extracted label</span>
                examples_from_this_class = train_examples{train_labels == this_class, :};
                <span class="comment">% adds the mean of that label's training examples to the 'means' array</span>
                m.means{end+1} = mean(examples_from_this_class);
                <span class="comment">% adds the std of that label's training examples to the 'stds' array</span>
                m.stds{end+1} = std(examples_from_this_class);

            <span class="keyword">end</span>

            <span class="comment">% empty cell array to store how often each label occurs</span>
            <span class="comment">% in the all the training examples as a decimal</span>
            <span class="comment">% (Naive Bayes assumes dependency between true class label and observed features)</span>
            m.priors = [];

            <span class="comment">% loops through all of the unique training labels</span>
            <span class="keyword">for</span> i = 1:m.n_classes

                <span class="comment">% extracts first label from the array of unique labels</span>
				this_class = m.unique_classes(i);
                <span class="comment">% extracts all training examples which belong to that extracted label</span>
                examples_from_this_class = train_examples{train_labels == this_class, :};
                <span class="comment">% calculates the number of the current label's training examples</span>
                <span class="comment">% in comparison to the total number of training examples as decimal</span>

                <span class="comment">% the decimal of that label's occurrence is added to the 'priors' array</span>
                m.priors(end+1) = size(examples_from_this_class, 1) / size(train_labels,1);

            <span class="keyword">end</span>

        <span class="keyword">end</span>

        <span class="comment">% the more training examples the longer training phase...</span>
        <span class="comment">% classification phase time is independent of number of training examples...</span>
        <span class="keyword">function</span> predictions = predict(m, test_examples)

            predictions = categorical; <span class="comment">% empty categorical array to store predicitons</span>

            <span class="comment">% loops through all examples using the height of the test examples data</span>
            <span class="keyword">for</span> i=1:size(test_examples,1)

				<span class="comment">% prints the number of the current test example being classified</span>
                fprintf(<span class="string">'classifying example %i/%i\n'</span>, i, size(test_examples, 1));
                <span class="comment">% extracts one example from test examples data</span>
                this_test_example = test_examples{i,:};
                <span class="comment">% each test example is passed through the predict_one function</span>
                this_prediction = mynb.predict_one(m, this_test_example);
                <span class="comment">% adds the prediction to the end of the 'predictions' categorical array</span>
                predictions(end+1) = this_prediction;
			<span class="keyword">end</span>
        <span class="keyword">end</span>

        <span class="keyword">function</span> prediction = predict_one(m, this_test_example)

            <span class="comment">% loops through all of the unique training labels</span>
            <span class="keyword">for</span> i=1:m.n_classes

                <span class="comment">% calculates the extracted test example 'Normal Distribution' based on the current label</span>
                this_likelihood = mynb.calculate_likelihood(m, this_test_example, i);
                <span class="comment">% gets the name of the current label/class</span>
                this_prior = mynb.get_prior(m, i);
                <span class="comment">% calculates the probability of the current label/class</span>
                <span class="comment">% using the likelihood &amp; prior for the current label/class</span>
                posterior_(i) = this_likelihood * this_prior;
            <span class="keyword">end</span>

            <span class="comment">% shows the largest value in the 'posterior_' array and its index</span>
            [winning_value_, winning_index] = max(posterior_);
            <span class="comment">% predicted label = the index of the max value in the 'posterior_' array</span>
            <span class="comment">% within the unique labels/classes array</span>
            prediction = m.unique_classes(winning_index);

        <span class="keyword">end</span>

        <span class="keyword">function</span> likelihood = calculate_likelihood(m, this_test_example, class)

			likelihood = 1; <span class="comment">% sets the default likelihood to 1</span>
            <span class="comment">% loops through the extracted test example's columns</span>
			<span class="keyword">for</span> i=1:length(this_test_example)
                <span class="comment">% calculates 'Normal Distribution' of each column within the test example against the current label</span>
                likelihood = likelihood * mynb.calculate_pd(this_test_example(i), m.means{class}(i), m.stds{class}(i));
            <span class="keyword">end</span>

        <span class="keyword">end</span>

        <span class="keyword">function</span> prior = get_prior(m, class)

            <span class="comment">% returns the occurrence of the specified class within the</span>
            <span class="comment">% training examples as a decimal/fraction</span>
			prior = m.priors(class);

        <span class="keyword">end</span>

        <span class="comment">% calculates the 'Probability Density' / 'Normal Distributions' (never 0)</span>
        <span class="keyword">function</span> pd = calculate_pd(x, mu, sigma)
			first_bit = 1 / sqrt(2*pi*sigma^2);
            second_bit = - ( ((x-mu)^2) / (2*sigma^2) );
            pd = first_bit * exp(second_bit);

		<span class="keyword">end</span>

    <span class="keyword">end</span>

<span class="keyword">end</span>
</pre><pre class="codeoutput">
ans = 

  mynb with no properties.

</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2019b</a><br></p></div><!--
##### SOURCE BEGIN #####
classdef mynb
    
    methods(Static)
        
        function m = fit(train_examples, train_labels)
            
            % array of all unique training labels
            m.unique_classes = unique(train_labels);
            % calculates the number of the unique labels
            m.n_classes = length(m.unique_classes);
            % mean & standard deviation used to generate the 
            % 'Probability Density' / 'Normal Distribution'
            m.means = {}; % empty cell array to store the mean
            m.stds = {}; % empty cell array to store the standard deviation
            
            % loops through all of the unique training labels
            for i = 1:m.n_classes
                
                % extracts first label from the array of unique labels
				this_class = m.unique_classes(i);
                % extracts all training examples which belong to that extracted label
                examples_from_this_class = train_examples{train_labels == this_class, :};
                % adds the mean of that label's training examples to the 'means' array
                m.means{end+1} = mean(examples_from_this_class);
                % adds the std of that label's training examples to the 'stds' array
                m.stds{end+1} = std(examples_from_this_class);
                
            end
            
            % empty cell array to store how often each label occurs 
            % in the all the training examples as a decimal
            % (Naive Bayes assumes dependency between true class label and observed features) 
            m.priors = []; 
            
            % loops through all of the unique training labels
            for i = 1:m.n_classes
                
                % extracts first label from the array of unique labels
				this_class = m.unique_classes(i);
                % extracts all training examples which belong to that extracted label
                examples_from_this_class = train_examples{train_labels == this_class, :};
                % calculates the number of the current label's training examples 
                % in comparison to the total number of training examples as decimal
                
                % the decimal of that label's occurrence is added to the 'priors' array
                m.priors(end+1) = size(examples_from_this_class, 1) / size(train_labels,1);
			
            end

        end

        % the more training examples the longer training phase...
        % classification phase time is independent of number of training examples...
        function predictions = predict(m, test_examples)

            predictions = categorical; % empty categorical array to store predicitons

            % loops through all examples using the height of the test examples data
            for i=1:size(test_examples,1)

				% prints the number of the current test example being classified
                fprintf('classifying example %i/%i\n', i, size(test_examples, 1));
                % extracts one example from test examples data
                this_test_example = test_examples{i,:};
                % each test example is passed through the predict_one function
                this_prediction = mynb.predict_one(m, this_test_example);
                % adds the prediction to the end of the 'predictions' categorical array 
                predictions(end+1) = this_prediction;
			end
        end

        function prediction = predict_one(m, this_test_example)

            % loops through all of the unique training labels
            for i=1:m.n_classes

                % calculates the extracted test example 'Normal Distribution' based on the current label
                this_likelihood = mynb.calculate_likelihood(m, this_test_example, i);
                % gets the name of the current label/class
                this_prior = mynb.get_prior(m, i);
                % calculates the probability of the current label/class
                % using the likelihood & prior for the current label/class
                posterior_(i) = this_likelihood * this_prior;
            end
            
            % shows the largest value in the 'posterior_' array and its index
            [winning_value_, winning_index] = max(posterior_);
            % predicted label = the index of the max value in the 'posterior_' array 
            % within the unique labels/classes array
            prediction = m.unique_classes(winning_index);

        end
        
        function likelihood = calculate_likelihood(m, this_test_example, class)
            
			likelihood = 1; % sets the default likelihood to 1
            % loops through the extracted test example's columns
			for i=1:length(this_test_example)
                % calculates 'Normal Distribution' of each column within the test example against the current label
                likelihood = likelihood * mynb.calculate_pd(this_test_example(i), m.means{class}(i), m.stds{class}(i));
            end
            
        end
        
        function prior = get_prior(m, class)
            
            % returns the occurrence of the specified class within the
            % training examples as a decimal/fraction
			prior = m.priors(class);
        
        end
        
        % calculates the 'Probability Density' / 'Normal Distributions' (never 0)
        function pd = calculate_pd(x, mu, sigma)
			first_bit = 1 / sqrt(2*pi*sigma^2);
            second_bit = - ( ((x-mu)^2) / (2*sigma^2) );
            pd = first_bit * exp(second_bit);
            
		end
            
    end
    
end
##### SOURCE END #####
--></body></html>